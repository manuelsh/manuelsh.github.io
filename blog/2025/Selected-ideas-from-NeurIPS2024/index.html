<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Selected ideas from NeurIPS 2024 | Manuel Sánchez Hernández </title> <meta name="author" content="Manuel Sánchez Hernández"> <meta name="description" content="NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas presented."> <meta name="keywords" content="machine-learning, artificial-intelligence, physics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://manuelsh.github.io/blog/2025/Selected-ideas-from-NeurIPS2024/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Selected ideas from NeurIPS 2024",
            "description": "NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas presented.",
            "published": "February 01, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Manuel Sánchez Hernández </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Selected ideas from NeurIPS 2024</h1> <p>NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas presented.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#agents-the-next-frontier">Agents, the next frontier</a> </div> <ul> <li> <a href="#conquering-human-user-interfaces">Conquering human user interfaces</a> </li> <li> <a href="#other-useful-resources-about-agents">Other useful resources about Agents</a> </li> </ul> <div> <a href="#building-and-improving-large-language-models">Building and improving Large Language Models</a> </div> <ul> <li> <a href="#are-we-running-out-of-data">Are we running out of data?</a> </li> <li> <a href="#architectures-and-rl-methods">Architectures and RL methods</a> </li> </ul> <div> <a href="#measuring-the-performance-of-foundation-models">Measuring the performance of foundation models</a> </div> <ul> <li> <a href="#benchmarks-to-advance-ai">Benchmarks to advance AI</a> </li> <li> <a href="#eureka-a-comprehensive-framework-to-evaluate-llms">EUREKA: A comprehensive framework to evaluate LLMs</a> </li> </ul> <div> <a href="#unified-representations-shedding-light-on-the-black-box">Unified Representations, shedding light on the black box</a> </div> <ul> <li> <a href="#the-platonic-representation">The platonic representation</a> </li> <li> <a href="#reverse-engineerig-intelligence">Reverse engineerig intelligence</a> </li> </ul> <div> <a href="#ai-safety-advocating-for-tools-not-agents">AI Safety: advocating for tools, not agents</a> </div> <div> <a href="#foundation-models-for-e-commerce">Foundation models for E-commerce</a> </div> <div> <a href="#concluding-remarks">Concluding remarks</a> </div> </nav> </d-contents> <p><span style="color: grey; font-weight: 300; font-size: 0.9em;">1st February 2025</span> <a href="https://neurips.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">NeurIPS</a> is widely considered <em>the</em> major AI research conference. With over 16,000 participants, 56 workshops, countless parallel tracks, and a staggering 3,650 posters, this event is more than just a conference: it offers a privileged vantage point into the state of the art in the field and their current challenges. The 2024 edition was hosted in Vancouver, during 6 packed days.</p> <p>While it’s impossible to capture its vast scope in a single post, here’s a glimpse of the most exciting ideas that stood out to me.</p> <h1 id="agents-the-next-frontier">Agents, the next frontier</h1> <p>Advancing intelligent systems requires shifting focus from standalone models (e.g., LLMs) to more complex, agent-based architectures capable of autonomous reasoning and decision-making. See for example the latest releases of frontier labs, such as o1 from OpenAI, DeepSeek, etc, where agentic methods like Chain of Thought are becoming more common.</p> <p>There were many interesting presentations on the topic, including showcases of agentic libraries, such as<a href="https://neurips.cc/Expo/Conferences/2024/workshop/100326" rel="external nofollow noopener" target="_blank">Autogen</a>, presented by Microsoft, or <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100321" rel="external nofollow noopener" target="_blank">Llama Stack</a>, by the folks of Meta.</p> <h2 id="conquering-human-user-interfaces">Conquering human user interfaces</h2> <p><a href="https://microsoft.github.io/OmniParser/" rel="external nofollow noopener" target="_blank">OmniParser</a>, also from Microsoft, is a promising framework that provides more information to a multimodal LLM about the content of a screen or browser, drastically facilitating the interaction of the model with user interfaces. Solving this problem is key to have true agents in our phones or computers.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/omniparser_example-480.webp 480w,/assets/img/blog_images/omniparser_example-800.webp 800w,/assets/img/blog_images/omniparser_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/omniparser_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of the output of Omniparser from a screenshot with Google Slides. </div> <p>A key limitation, as I have observed in my own tests, is that feeding raw screenshots or HTML to a multimodal LLM (e.g., GPT4V) and granting it control over the mouse and keyboard results in poor performance on UI-driven tasks like booking flights or hotels. This is reflected in the low accuracy on GUI task oriented benchmarks, such as the <a href="https://web.archive.org/web/20250317225153/https://paperswithcode.com/sota/natural-language-visual-grounding-on" rel="external nofollow noopener" target="_blank">ScreenSpot benchmark</a>, where GPT4V arrives to 16% accuracy.</p> <p>However, if the model is supplemented with the input coming from Omniparser, accuracy jumps to 73% on the same dataset. This has been surpassed <a href="https://web.archive.org/web/20250317225153/https://paperswithcode.com/sota/natural-language-visual-grounding-on" rel="external nofollow noopener" target="_blank">by other models</a>, which means that in less than one year, we will likely see AI operating seamlessly with the UI of our phones or computers as humans do.</p> <h2 id="other-useful-resources-about-agents">Other useful resources about Agents</h2> <p>The folks of Meta showed how to build agents with their <a href="https://github.com/meta-llama/llama-stack" rel="external nofollow noopener" target="_blank">Llama Stack</a>, providing also a great <a href="https://colab.research.google.com/drive/1F2ksmkoGQPa4pzRjMOE6BXWeOxWFIW6n#scrollTo=K4AvfUAJZOeS" rel="external nofollow noopener" target="_blank">notebook with many relevant examples</a>, which includes RAG evaluation with LLM as a judge.</p> <p>A key highlight was their proposed agentic architecture, which features a central executor coordinating all operations, as illustrated below. Just follow the numbers in order to better understand it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/Meta_agentic_archirecture_proposal.JPG" sizes="95vw"></source> <img src="/assets/img/blog_images/Meta_agentic_archirecture_proposal.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Architecture of an agent as proposed by Meta. Source: Active Training: Building Agentic Apps with Llama 3.2 and Llama Stack. Neurips 2024. </div> <p>They also provided some hints on which model size to use for different tasks. The following table is quite useful.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/Meta_model_sizes_and_usages_table.JPG" sizes="95vw"></source> <img src="/assets/img/blog_images/Meta_model_sizes_and_usages_table.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table showing the model sizes and their usages as proposed by Meta. Source: Active Training: Building Agentic Apps with Llama 3.2 and Llama Stack. Neurips 2024. </div> <h1 id="building-and-improving-large-language-models">Building and improving Large Language Models</h1> <p>One of the standout topics at NeurIPS this year was the process of building and improving Large Language Models. A particularly noteworthy presentation was given by the AllenAI team, who provided a <a href="https://neurips.cc/virtual/2024/tutorial/99526" rel="external nofollow noopener" target="_blank">detailed overview of the end-to-end process of building an LLM</a>. From data acquisition to post-training, they shared many insights and tips. This topic is so rich that it deserves a summary of its own, which you can find <a href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/">here</a>.</p> <h2 id="are-we-running-out-of-data">Are we running out of data?</h2> <p>A recurring theme was data scarcity. Kyle Lo from AllenAI clarified that while data itself isn’t vanishing, open-access data is becoming increasingly limited. Ilya Sutskever, in his remarks upon receiving the “Test of Time Award” for his paper, described data as the “fossil fuel of AI,” noting that while compute continues to grow, data is not growing at the same pace. He suggested that we should be looking at “synthetic data,” inference time compute, and agents as potential solutions.</p> <p>This was challenged by Jason Weston, who pointed out that significant portion of the training of LLMs in frontier companies relies on “closed data,” which they possess and are generating in substantial quantities. He expressed skepticism about the severity of the data scarcity issue, suggesting that Ilya’s perspective might be influenced by his recent departure from OpenAI and the resulting loss of access to that data.</p> <p>It is worth mentioning the work of Epoch AI. In their <a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data" rel="external nofollow noopener" target="_blank">Will We Run Out of Data?</a> paper they project that human public text, estimated in 300 trillion tokens, will be fully utilized between 2026 and 2032, or earlier (see chart below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/epoch-ai-data-utilization-480.webp 480w,/assets/img/blog_images/epoch-ai-data-utilization-800.webp 800w,/assets/img/blog_images/epoch-ai-data-utilization-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/epoch-ai-data-utilization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Source: Epoch AI, June 2024 </div> <p>Epoch AI focuses in this study on textual data. However, a significant portion of data exists in other formats, such as images, audio, and video, which can also be used for training.</p> <p>While computational power grows exponentially and data increases at a linear rate, algorithms and methods continue to become more efficient. Furthermore, alternatives like self-distillation (model generates data and trains with it), <a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback" rel="external nofollow noopener" target="_blank">Constitutional AI</a>, synthetic data, and private datasets reduce this reliance on high data volume. For all these reasons I don’t think data will be the main bottleneck.</p> <h2 id="architectures-and-rl-methods">Architectures and RL methods</h2> <p>It’s clear that other alternatives to the Transformer architecture are standing out, such as <a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">Mamba</a> or <a href="https://neurips.cc/virtual/2024/invited-talk/101129" rel="external nofollow noopener" target="_blank">xLSTM</a>. These architectures are more efficient that the Transformer at inference, as the computing doesn’t grow quadratically with the number of input tokens, while they can parallelize the prediction of the next token in the training, like the Transformer does, instead of sequentially like previous architectures (RNN, LSTM…). Sepp Hochreiter, the creator of LSTM, presented xLSTM, acknowledging its strong resemblance to Mamba.</p> <p>However, although these arechitectures were mentioned many times, in practice they are not yet widely used.</p> <p>Additionally, <a href="https://huggingface.co/blog/rlhf" rel="external nofollow noopener" target="_blank">Reinforcement Learning with Human Feedback</a> (or RLHF), which is the method used by OpenAI ChatGPT to make a language model a chatbot, is being substituted or supplemented by many other methods, like DPO, which is significantly easier and performs at a similar level. More details in my summary on <a href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/">Opening the LLM pipeline</a>.</p> <h1 id="measuring-the-performance-of-foundation-models">Measuring the performance of foundation models</h1> <p>Although benchmarking models is part of building models, this topic is so important that requires its own section. Benchmarking is not only important to understand how well a model performs but building relevant benchmarks is key to advance the field.</p> <h2 id="benchmarks-to-advance-ai">Benchmarks to advance AI</h2> <p>The definition of intelligence is ellusive, that is why those benchmarks that are easy for humans but hard for AI models are critical, as they establish a new baseline to beat. One of them is <a href="https://arcprize.org/" rel="external nofollow noopener" target="_blank">ARC</a>, which requires the ML model to solve a series of puzzles, each one with a different logic, like the one shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/ARC-benchmark_example.svg" sizes="95vw"></source> <img src="/assets/img/blog_images/ARC-benchmark_example.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of an ARC puzzle. Source: https://lab42.global/arc/ </div> <p>The performance of humans in the ARC test is very high, around 90% accuracy according to the ARC team, while at the time of Neurips 2024 the best model was at 53%. Interestingly, less than a week after François Chollet presentation in NeurIPS, OpenAI announced that their <a href="https://web.archive.org/web/20250323011500/https://community.openai.com/t/day-12-of-shipmas-new-frontier-models-o3-and-o3-mini-announcement/1061818" rel="external nofollow noopener" target="_blank">new o3 model</a> is able to reach to 76% in ARC. A great example of how quickly the field moves!</p> <p><a href="https://melaniemitchell.me/" rel="external nofollow noopener" target="_blank">Melanie Mitchell</a>, from the Santa Fe Institute, also showed during a workshop about <a href="https://neurips.cc/virtual/2024/workshop/84749" rel="external nofollow noopener" target="_blank">System-2 reasoning</a> how current state of the art LLMs fail when some benchmarks are modified in trivial ways. She mentioned an example of the paper <a href="https://arxiv.org/pdf/2307.02477" rel="external nofollow noopener" target="_blank">Reasoning or Reciting?</a> where in a Python code benchmark, where GPT4 can perform very well, just by introducing a simple change in the way the language works (“now lists index start with one and not zero) the model performance drops drastically. See the chart below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/reasoning-or-reciting-benchmark-paper-neurips-post-480.webp 480w,/assets/img/blog_images/reasoning-or-reciting-benchmark-paper-neurips-post-800.webp 800w,/assets/img/blog_images/reasoning-or-reciting-benchmark-paper-neurips-post-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/reasoning-or-reciting-benchmark-paper-neurips-post.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> GPT4 performance on the default version of various benchmarks and in the modified version (counterfactuals). Source: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. Wu et al. March 2024. </div> <p>Building “easy for humans, hard for AI” kind of benchmarks are key to the development of more intelligent models. Indeed, as Fei Fei pointed out in <a href="https://neurips.cc/virtual/2024/invited-talk/101127" rel="external nofollow noopener" target="_blank">her inspiring presentation</a> (highly recommended!), the ImageNet benchmark that she created was a key element for the rebirth of neural networks in 2012, and the newly coined term “Deep Learning”.</p> <h2 id="eureka-a-comprehensive-framework-to-evaluate-llms">EUREKA: A comprehensive framework to evaluate LLMs</h2> <p>The folks from Microsoft <a href="https://neurips.cc/Expo/Conferences/2024/talk%20panel/105693" rel="external nofollow noopener" target="_blank">presented</a> a comprehensive and open source framework to evaluate multimodal and language models called <a href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/" rel="external nofollow noopener" target="_blank">Eureka</a>, which assesses the performance of models across several dimensions. Some of the main conclussions of their evaluation of 14 large foundation models are:</p> <ul> <li>Models like Claude 3.5 Sonnet, GPT-4o 2024-05-13, and Llama 3.1 405B show distinct strengths in specific tasks but are not universally superior across all benchmarks. This highlights the need for task-specific analysis rather than assuming a model’s overall superiority.</li> <li>Current AI models struggle significantly with multimodal tasks, particularly those requiring detailed image understanding and spatial reasoning. For example, all models perform poorly on Object Detection.</li> </ul> <p>In the folllowing chart you check see the results for both language and multimodal tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/eureka-benchmark-framework-neurips-post-480.webp 480w,/assets/img/blog_images/eureka-benchmark-framework-neurips-post-800.webp 800w,/assets/img/blog_images/eureka-benchmark-framework-neurips-post-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/eureka-benchmark-framework-neurips-post.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Performance of best and worse models for multimodal (left) and language (right) datasets in in Eureka-Bench. Note the room to improve in Object Detection, Information Retrieval or navigation. Source: Eureka: Evaluating and Understanding Progress in AI. Microsoft Research, NeurIPS 2024. </div> <h1 id="unified-representations-shedding-light-on-the-black-box">Unified Representations, shedding light on the black box</h1> <p>Significant progress have been done on the topic of understanding how neural networks (human and artificial) encodes and process information. Several illuminating ideas around the topic were presented in the <a href="https://neurips.cc/virtual/2024/workshop/84701" rel="external nofollow noopener" target="_blank">UniReps Workshop</a> at NeurIPS 2024.</p> <h2 id="the-platonic-representation">The platonic representation</h2> <p>We have substantial evidence that different neural networks, including artificial and human neural networks, converge towards the same way of representing the world. This evidence comes by looking at the multidimensional spaces that the activations of the layers of neural networks produce (an embedding) when a concept is used as input. In <a href="https://arxiv.org/abs/2405.07987" rel="external nofollow noopener" target="_blank">The Platonic Representation Hypothesis</a> paper, authors observed that the spaces generated by embeddings of different models have very similar characteristics: for example the distances between points of the same concepts (e.g. distance between the concepts <em>pear</em> and <em>giraffe</em>) in a language model or in a vision model remain very similar, and this similarity increases the better the models are. See chart below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/alignment_unified_representations_neurips_2024-480.webp 480w,/assets/img/blog_images/alignment_unified_representations_neurips_2024-800.webp 800w,/assets/img/blog_images/alignment_unified_representations_neurips_2024-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/alignment_unified_representations_neurips_2024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The better the models, the more aligned are their representations. Source: Phillip Isola, Unireps, NeurIPS 2024 </div> <p>Not only that, but there is evidence that the same happens with the activation of our neurons in our brain. They also generate a space that is similar to the ones of the frontier models, and we can even use LLM to interpret the output of this human brain activations.</p> <p>A profound question arises: is there a unique <em>platonic representation</em> that models and humans converge to? Knowing it could help building more intelligent models. If you find this material interesting, I recommend reading <a href="https://phillipi.github.io/prh/#what_converging_to" rel="external nofollow noopener" target="_blank">the summary of the paper</a>.</p> <p>Having internal representations encoded as perpendicular vectors also leads to conclude that a neural computation is a transformation of a representation into another representation. That’s the job of the neural network weight and biases, to transform the input representation (usually in the form of learned embeddings) into another representation that is useful for the task at hand. Incredible the power of linear algebra and some non-linearities!</p> <h2 id="reverse-engineerig-intelligence">Reverse engineerig intelligence</h2> <p>Many other talks in this workshop where about gaining a further understanding of the mechanics of the brain and neural networks. For example, they discovered that <a href="https://arxiv.org/pdf/2409.05771" rel="external nofollow noopener" target="_blank">middle layers of LLMs are better to predict</a> the concepts behind human brain activations. There is evidence that LLMs, in order to predict the next token, generate first an internal representation that encodes many functions of the language (which is in the middle layers), which is richer, versus the represenation that just predict the next token, in the final layers.</p> <p>There is also strong evidence that neural networks encode information in “directions” in a multidimensional space, where each useful abstract concept (for example, the language a text is written in) is encoded in a different direction, each one “almost” perpendicular to each other, which is possible in a multidimensional space (in a 3d space, there are only 3 dimenspossible perpendicular vectors, but in higher dimensions space, if we relax the constraint of perpendicularity from 90 degrees angle to 89-91 degrees, the amount of almost perpendicular vectors grow exponentially with the number of dimensions). Highly recommended to watch this lesson of ThreeBlueOneBrown on <a href="https://www.3blue1brown.com/lessons/mlp" rel="external nofollow noopener" target="_blank">How might LLM store facts</a>. In fact, watch all the Deep Learning videos of this channel, they are the best I’ve seen explaining the concepts of the transformer.</p> <p>Very interesting also the Mechanistic Interpretability talk of <a href="https://www.neelnanda.io/" rel="external nofollow noopener" target="_blank">Neel Nanda</a>, from DeepMind. Mechanistic Interpretability aspires to reverse engineer neural networks, working on the hypothesis that models learn human comprehensible structures that can be understood. He showed an example where they are able to identify a “direction in space” that encodes refusal, i.e. when the model refuses to speak about certain topic, usually because of the safety constraints. Knowing this direction, they are able to deactivate it, just by subtracting that vector, allowing the model to respond on originally unintended ways. This refusal direction appears in every model they checked, is almost universal.</p> <p>One clear application of better understanding the inner workings of neural networks is to improve their safety. Which lead us to the next topic.</p> <h1 id="ai-safety-advocating-for-tools-not-agents">AI Safety: advocating for tools, not agents</h1> <p><a href="https://yoshuabengio.org/" rel="external nofollow noopener" target="_blank">Youshua Bengio</a> and <a href="https://physics.mit.edu/faculty/max-tegmark/" rel="external nofollow noopener" target="_blank">Max Tegmark</a> participated in a <a href="https://neurips.cc/virtual/2024/workshop/84705" rel="external nofollow noopener" target="_blank">relevant workshop on AI safety</a>. One of their main arguments was that AI’s benefits can be maximized while minimizing risks by developing specialized models rather than fully autonomous agents. A great example of this is the <a href="https://deepmind.com/research/case-studies/alphafold" rel="external nofollow noopener" target="_blank">AlphaFold</a> model, which is a tool that helps scientists to predict the 3D structure of proteins; key for drug discovery and currently widely used.</p> <h1 id="foundation-models-for-e-commerce">Foundation models for E-commerce</h1> <p>The folks from Shopify presented an initiative to build a <a href="https://neurips.cc/Expo/Conferences/2024/talk%20panel/100357" rel="external nofollow noopener" target="_blank">foundation model for e-commerce</a>, which takes a selection of events as inputs (these are the tokens), and try to predict following events. The idea is that such a model takes many functions of a typical e-commerce platform, like recommendedr system, fraud detection, next best intervention, etc.</p> <p>They presented a couple of architecture choices to address the problem, <a href="https://arxiv.org/abs/2402.17152v1" rel="external nofollow noopener" target="_blank">HSTU</a>, and <a href="https://arxiv.org/abs/2305.05065" rel="external nofollow noopener" target="_blank">TIGER</a>. What is promising about their work is that they mention an uplift of 240-480% recall@10 in <em>offline experiments</em>. I am looking forward to see the results once models are deployed in production.</p> <h1 id="concluding-remarks">Concluding remarks</h1> <p>The scale of NeurIPS 2024 is a testament to the rapid growth of the field of AI. The conference showcased a wide range of ideas and approaches, from the development of large language models to the exploration of new architectures and reinforcement learning methods. The presentations on agents and the development of tools for AI safety were particularly thought-provoking, highlighting the potential for AI to transform our world in the coming years.</p> <p>As a final thought, consider that human intelligence, the most advanced we know (so far), processes 50-100 terabytes of sensory data annually, all powered by a brain consuming just ~20 Watts. This sets an ambitious benchmark for AI systems to aspire to.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'manuelsh/manuelsh.github.io',
        'data-repo-id': 'R_kgDONjpQdw',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDONjpQd84Clz9W',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'above',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Manuel Sánchez Hernández. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-C3FNF06T6P"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-C3FNF06T6P');
  </script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>