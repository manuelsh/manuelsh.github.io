<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ideas from NeurIPS 2024 | Manuel Sánchez Hernández </title> <meta name="author" content="Manuel Sánchez Hernández"> <meta name="description" content="NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas from the conference."> <meta name="keywords" content="machine-learning, artificial-intelligence, physics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://manuelsh.github.io/blog/2025/Ideas-from-NeurIPS2024/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Ideas from NeurIPS 2024",
            "description": "NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas from the conference.",
            "published": "January 17, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Manuel Sánchez Hernández </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Ideas from NeurIPS 2024</h1> <p>NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas from the conference.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#agents-the-next-frontier">Agents, the next frontier</a> </div> <ul> <li> <a href="#conquering-human-user-interfaces">Conquering human user interfaces</a> </li> <li> <a href="#other-useful-resources-about-agents">Other useful resources about Agents</a> </li> </ul> <div> <a href="#building-and-improving-large-language-models">Building and improving Large Language Models</a> </div> <ul> <li> <a href="#are-we-running-out-of-data">Are we running out of data?</a> </li> </ul> </nav> </d-contents> <p><a href="https://neurips.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">NeurIPS</a> is widely considered <em>the</em> major AI research conference. With over 16,000 participants, 56 workshops, countless parallel tracks, and a staggering 3,650 posters, it’s not just an event—it’s an intellectual marathon. This year 2024, Vancouver, Canada, hosted it, and during six packed days, it offered a privileged vantage point into the state of the art in the field and their current challenges. While it’s impossible to capture its vast scope in a single post, here’s a glimpse of the most exciting ideas that stood out to me.</p> <p>I will divide it into xxx main themes: [[[put them here]]]</p> <h1 id="agents-the-next-frontier">Agents, the next frontier</h1> <p>Many researchers agree that to bring intelligent systems to the next level we need to focus on “Agents”, instead of just models (such as LLMs). This was mentioned by Ilya Sutskever in his presentations and there were many interesting presentations on the topic, including showcases of agentic libraries, like <a href="https://www.microsoft.com/en-us/research/project/autogen/" rel="external nofollow noopener" target="_blank">Autogen</a>, presented by Microsoft, and <a href="https://github.com/meta-llama/llama-stack" rel="external nofollow noopener" target="_blank">Llama Stack</a>, by the folks of Meta.</p> <h2 id="conquering-human-user-interfaces">Conquering human user interfaces</h2> <p>Very promising the <a href="https://microsoft.github.io/OmniParser/" rel="external nofollow noopener" target="_blank">OmniParser method</a>, also from Microsoft, which can be used to build agents able to interact with a screen or browser. In a future where our interactions with the digital world are intermediated by AI agents, they should be able to interact with the content as we do, using keyboard, mouse or tapping in a screen to navigate through user interfaces.</p> <p>For example, as I have tested myself, giving the LLM the control of the mouse and keyboard and the ability to use a browser (easily done by asking the LLM to build code with <a href="https://www.selenium.dev/" rel="external nofollow noopener" target="_blank">Selenium</a> or similar libraries), if you ask it to do certain tasks, like book a flight, fails miserably, with accuracies around 16% in the ScreenSpot benchmark.</p> <p>But if the model is supplemented with the input coming from Omniparser, this accuracy jumps to 73% on the same dataset. Though this increase is very significant, it has already been surpassed <a href="https://paperswithcode.com/sota/natural-language-visual-grounding-on" rel="external nofollow noopener" target="_blank">by other models</a>, which means that in less than one year, we will likely see AI operating seamlessly with the UI of our phones or computers as humans do.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/omniparser_example-480.webp 480w,/assets/img/blog_images/omniparser_example-800.webp 800w,/assets/img/blog_images/omniparser_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/omniparser_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of the output of Omniparser from a screenshot with Google Slides. </div> <h2 id="other-useful-resources-about-agents">Other useful resources about Agents</h2> <p>The folks of Meta showed how to build agents with their <a href="https://github.com/meta-llama/llama-stack" rel="external nofollow noopener" target="_blank">Llama Stack</a>, providing also a great <a href="https://colab.research.google.com/drive/1F2ksmkoGQPa4pzRjMOE6BXWeOxWFIW6n#scrollTo=K4AvfUAJZOeS" rel="external nofollow noopener" target="_blank">notebook with many relevant examples</a>, which includes RAG evaluation with LLM as a judge.</p> <p>A couple of highlights were their agentic proposed architecture where a central executor orchestrates everything, as shown in the image below. Just follow the numbers in order to better understand it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/Meta_agentic_archirecture_proposal.JPG" sizes="95vw"></source> <img src="/assets/img/blog_images/Meta_agentic_archirecture_proposal.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Architecture of an agent as proposed by Meta. Source: Active Training: Building Agentic Apps with Llama 3.2 and Llama Stack. Neurips 2024. </div> <p>They also provided some hints on which model size to use for different tasks. I found the following table quite useful.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/Meta_model_sizes_and_usages_table.JPG" sizes="95vw"></source> <img src="/assets/img/blog_images/Meta_model_sizes_and_usages_table.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table showing the model sizes and their usages as proposed by Meta. Source: Active Training: Building Agentic Apps with Llama 3.2 and Llama Stack. Neurips 2024. </div> <h1 id="building-and-improving-large-language-models">Building and improving Large Language Models</h1> <p>One of the standout topics at NeurIPS this year was the process of building and improving Large Language Models. A particularly noteworthy presentation was given by the AllenAI team, who provided a detailed overview of the end-to-end process of building an LLM. From data acquisition to post-training, they shared many insights and tips. This topic is so rich that it deserves a summary of its own, which you can find <a href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/">here</a>.</p> <h2 id="are-we-running-out-of-data">Are we running out of data?</h2> <p>Many speakers touched on the topic of data scarcity. Kyle Lo from AllenAI mentioned that we are not running out of data, but of <em>open</em> data. Ilya Sutskever, in his remarks upon receiving the “Test of Time Award” for his paper, described data as the “fossil fuel of AI,” noting that while compute continues to grow, data is not growing at the same pace. He suggested that we should be looking at “synthetic data,” inference time compute, and agents as potential solutions.</p> <p>This was challenged by Jason Weston (vibrant multi-colored hair today), who pointed out that significant portion of the training of LLMs in frontier companies relies on “closed data,” which they possess and are generating in substantial quantities. He expressed skepticism about the severity of the data scarcity issue, suggesting that Ilya’s perspective might be influenced by his recent departure from OpenAI and the resulting loss of access to that data.</p> <p>It is worth mentioning the work of Epoch AI on precisely this topic. In their <a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data" rel="external nofollow noopener" target="_blank">Will We Run Out of Data?</a> paper they project that human public text, estimated in 300 trillion tokens, will be fully utilized between 2026 and 2032, or earlier (see chart below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/epoch-ai-data-utilization-480.webp 480w,/assets/img/blog_images/epoch-ai-data-utilization-800.webp 800w,/assets/img/blog_images/epoch-ai-data-utilization-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog_images/epoch-ai-data-utilization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Source: Epoch AI, June 2024 </div> <p>Epoch AI focus here on textual data. However, a significant portion of data exists in other formats, such as images, audio, and video, which can also be used for training. While computational power grows exponentially and data increases at a linear rate, advancements in algorithms and methods continue to become more efficient. Furthermore, alternatives like self-distillation (model generates data and trains with it), <a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback" rel="external nofollow noopener" target="_blank">Constitutional AI</a>, synthetic data, and private datasets reduce this open data reliance. For all these reasons I don’t think data will be a blocker for the next years.</p> <p>Finally, consider that the most intelligent entities we know, humans, process an estimated 50-100 terabytes of raw data annually (through all our senses) with brain that consumes only ~20 Watts. This sets a benchmark on the efficiency of intelligence.</p> <h2 id="architectures-and-rl-methods">Architectures and RL methods</h2> <p>It’s clear that other alternatives to the Transformer architecture are standing out, such us <a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">Mamba</a> or <a href="https://arxiv.org/abs/2405.04517" rel="external nofollow noopener" target="_blank">xLSTM</a>. These architectures are more efficient at inference, as the computing doesn’t grow quadratically with the number of input tokens, while they can parallelize the prediction of the next token in the training, like the Transformer does, instead of sequentially like previous architectures (RNN, LSTM…). xLSTM was presented by Hochreiter, creator of the LSTM architecture, which admitted that is very similar to Mamba: “architectures converge”. However, although they were mentioned many times, they are not clear winners yet.</p> <p>Also, Reinforcement Learning with Human Feedback (or RLHF), which is the method used by OpenAI ChatGPT to make a language model a chatbot, is being substituted or supplemented by many other methods, like DPO, which is significantly easier and performs at a similar level. More details in my summary on <a href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/">Opening the LLM pipeline</a>.</p> <h1 id="measuring-the-performance-of-foundation-models">Measuring the performance of foundation models</h1> <p>Although benchmarking models is part of building models, this topic is so relevant that requires its own section. Benchmarking is important for several reasons: to understand how good a model is but also to build more intelligent models.</p> <h2 id="benchmarks-to-advance-ai">Benchmarks to advance AI</h2> <p>Intelligence definition is ellusive, that is why those benchmarks that are easy for humans but hard for AI models are very relevant. One of them is <a href="https://arcprize.org/" rel="external nofollow noopener" target="_blank">ARC</a>, which requires the ML model to solve a series of puzzles, each one with a different logic, like the one shown below.</p> <p>[[[[[ADD EXAMPLE!!!]]]]]</p> <p>The performance of humansin the ARC test is very high, around 90% accuracy, while at the time of Neurips 2024 the best model was at 53%. Interestingly, less than a week after François Chollet presentation in NeurIPS, OpenAI announced that their <a href="https://community.openai.com/t/day-12-of-shipmas-new-frontier-models-o3-and-o3-mini-announcement/1061818" rel="external nofollow noopener" target="_blank">new o3 model</a> is able to reach to 76% in ARC. A great example of how quickly the field moves!</p> <p><a href="https://melaniemitchell.me/" rel="external nofollow noopener" target="_blank">Melanie Mitchell</a>, from the Santa Fe Institute, also showed during a [[[[tutorial/workshop/presentation on….]]]] how current state of the art LLM fail when performing some trivial modifications on benchmarks. She mentioned an example of the insightful paper <a href="https://arxiv.org/pdf/2307.02477" rel="external nofollow noopener" target="_blank">Reasoning or Reciting?</a> where in a Python code benchmark, which GPT4 can do very well, just by introducing a simple change in the way the language work (“now lists index start with 1 and not zero) the model fails misserably. See the chart below. This provides a good glimpse on how far are our current best transformer models to be considered AGI.</p> <p>[[[put image from paper]]]</p> <p>In reality, building “easy for humans, hard for AI” kind of benchmarks are key to the development of more intelligent models. Indeed, as Fei Fei pointed out in her inspiring presentation, building the ImageNet benchmark was key for the rebirth of neural networks in 2012, and the newly coined term “Deep Learning”.</p> <h2 id="eureka-a-comprehensive-framework-to-evaluate-llms">EUREKA: A comprehensive framework to evaluate LLMs</h2> <p>The folks from Microsoft presented a comprehensive and open source framework to evaluate multimodal and language models called <a href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/" rel="external nofollow noopener" target="_blank">Eureka</a>, which assesses the performance of models across several dimensions. Some of the main conclussions of their evaluation of 14 large foundation models are:</p> <ul> <li>Models like Claude 3.5 Sonnet, GPT-4o 2024-05-13, and Llama 3.1 405B show distinct strengths in specific tasks but are not universally superior across all benchmarks. This highlights the need for task-specific analysis rather than assuming a model’s overall superiority.</li> <li>Current AI models struggle significantly with multimodal tasks, particularly those requiring detailed image understanding and spatial reasoning. For example, all models perform poorly on Object Detection. In the folllowing chart you can see the results. They are quite insightful!</li> </ul> <p>[[[put image from paper]]]</p> <h1 id="representation-learning">Representation learning</h1> <ul> <li>Very cool workshop on representation learning where they have evidence about “different neural networks, trained different, including human neural networks, converge towards the same way of representing the world”.</li> </ul> <h1 id="ai-safety">AI Safety</h1> <p>Main idea: we can get all the benefits of AI without the risks by not doing agents but special purpose models.</p> <h1 id="practical-applications">Practical applications</h1> <p>Presentation of Shopify</p> <h1 id="drama">Drama</h1> <ul> <li>Women that insulted Chinese</li> <li>Price to the main paper with guy that ….</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'manuelsh/manuelsh.github.io',
        'data-repo-id': 'R_kgDONjpQdw',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDONjpQd84Clz9W',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'above',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Manuel Sánchez Hernández. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-C3FNF06T6P"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-C3FNF06T6P');
  </script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>