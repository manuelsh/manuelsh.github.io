<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://manuelsh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://manuelsh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-31T23:19:16+00:00</updated><id>https://manuelsh.github.io/feed.xml</id><title type="html">Manuel Sánchez Hernández</title><subtitle>I like to do machine learning at scale </subtitle><entry><title type="html">Opening the LLM pipeline</title><link href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/" rel="alternate" type="text/html" title="Opening the LLM pipeline"/><published>2025-01-03T12:00:00+00:00</published><updated>2025-01-03T12:00:00+00:00</updated><id>https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop</id><content type="html" xml:base="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/"><![CDATA[<p>This post summarizes a fanstic tutorial about building LLM, titled <a href="https://neurips.cc/virtual/2024/tutorial/99526">“Opening the Language Model Pipeline: A Tutorial on Data Preparation, Model Training, and Adaptation”</a> (<a href="https://docs.google.com/presentation/d/179dpzWSQ9G7EAUlvaJdeE0av9PLuk9Rl33nfhHSJ4xI/edit#slide=id.g30a4c7e9678_0_0">slides</a>). It was presented at NeurIPS 2024, by <a href="https://kyleclo.com/">Kyle Lo</a>, <a href="https://akshitab.github.io/">Akshita Bhagia</a> and <a href="https://www.natolambert.com/">Nathan Lambert</a>, all from the <a href="https://allenai.org/">Allen Institute for AI</a>. I think it could be useful to share some of the main ideas.</p> <p>The process to build a Large Language Model is very involved, and the authors went through it end to end, providing many details and practical knowledge: starting with the data preparation, continuing wiht model training (also called pre-training), and adaptation (or post-training). Here I summarize the main takeaways from each part, with some additional notes added.</p> <h2 id="data-preparation">Data preparation</h2> <p>Data preparation mainly means data acquisition, data transformation (deduplication, quality control, etc) and data evaluation.</p> <h3 id="data-acquisition">Data acquisition</h3> <p>To acquire data, crawling is common. However, it’s <a href="https://www.dataprovenance.org/Consent_in_Crisis.pdf">becoming harder to crawl data</a>. Many websites are opting out or implementing anti crawlers protection, as shown in the figure below. Note that this will create a barrier to enter for new players. As Kyle Lo said: we are not running out of data, we are running out of <em>open</em> data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/crawling_data.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/crawling_data.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Longpre et. al. 2024. Consent in Crisis: The Rapid Decline of the AI Data Commons. Data Provenance Initiative. </div> <p>Crawling data from websites implies, for many of them, understanding the JavaScript logic, which in many cases is unique to the website. This can be challenging because each website may use different frameworks or obfuscation techniques, making it necessary to decipher custom implementations. For example, a site might load data dynamically through complex API calls embedded in asynchronous scripts, requiring tailored solutions for successful extraction. It also requires to parse the data from all the HTML, which is not easy. For PDF’s or scanned documents is also difficult, as many tools are not able to parse the data correctly.</p> <h3 id="data-transformation">Data transformation</h3> <p>That mostly means language filtering, deduplication, removing sensitive content (including private content) and ensuring a desired quality. In reality, it’s a classification problem, and can be done using machine learning with small classifiers. They recommend the library <a href="https://github.com/facebookresearch/fastText">fastText</a>, which is quite efficient (and used across the industry), although more involved classifiers can be used.</p> <p>They also shared the amount of data that remains once the data is filtered for quality and deduplication: reductions are usually of the order of 65 times.</p> <p>Filtering for quality can be, in some cases, problematics, as one can be undesiringly removing specific themes which are usually classified as lower quality, e.g. high school related content.</p> <p>It is also quite difficult to remove personal data, as it was highlighted by <a href="https://aclanthology.org/2023.trustnlp-1.18.pdf">Subramani et al (2023)</a>, where they showed that accuracies with simple Regex or tools like <a href="https://microsoft.github.io/presidio/">Presidio</a> can be quite low.</p> <h3 id="data-evaluation">Data evaluation</h3> <p>Finally, data must be evaluated by training models and running benchmarks, such as MMLU, HumanEval, GSM8K… The evaluation should be done systematically to each group of data, ideally starting with a smaller and cheaper mode. In general it is a very involved process with many nuances, like what is the best model size, measuring the effect of your data filtering, etc</p> <h3 id="a-new-trend-data-curriculum">A new trend: data curriculum</h3> <p>Some interesting new trend: “data curriculum”, which consists of, after training the model with trillions of tokens (high quantity, less quality), at the end of it one switches data to either very high quality sources, specific instructions or synthetic data.</p> <h2 id="model-training-or-pre-training">Model training (or pre-training)</h2> <p>Pre-training, the process where you train a LLM on next token prediction with a large amount of text, is currently mostly done with a Transformer architecture, accepting many different configurations that are successful, including many attention mechanisms (e.g. multi head, grouped-query or multi-query). See image below, how different configurations of the hyperparameters (marked in red) can lead to successful models.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/LLM_hyperparameter_configuration.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/LLM_hyperparameter_configuration.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Different configurations of the hyperparameters (marked in red) can lead to successful models. From the tutorial authors. </div> <p>In terms of scale the approximate good scaling law given by the <a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a> (compute budget approximately 6 times the number of parameters by data tokens, and data tokens approx 20 times the number of parameters), although in practice everybody keeps training further.</p> <p>In terms of costs, the pre-training can be very expensive, as it is very intensive in computational resources. See for example the table below, by the authors of the tutorials: a 7b parameter model and 150B tokens (just above the Chinchilla paper budget), will cost approx $10k.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/cost_of_LLM_training.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/cost_of_LLM_training.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Costs for different models and data sizes. From the tutorial authors. </div> <p>Common positional embeddings today are <a href="https://arxiv.org/abs/2104.09864">rotary positional embeddings</a> (RoPE) and the <a href="https://arxiv.org/abs/2002.05202v1">SwiGLU activation</a>, which, unlike ReLU activation, is smooth (differentiable) at zero.</p> <h3 id="problems-with-loss-function-convergence">Problems with loss function convergence</h3> <p>When the loss function spikes punctually, look at your data, probably is a low quality batch that needs to be filtered.</p> <p>When the loss function starts spiking and diverging, one needs to ensure that the scale of activations and gradients remain roughly the same, and they should scale with model width. Better to use normal initialization and RMSNorm, QK-Norm and change the order of the layer norm. Finally, ensure that the token embedding does not become too small (no weight decay).</p> <p>Run experiments with smaller models first, to find optimal parameters, decide on data ablations.</p> <h3 id="additional-tips-for-pre-training">Additional tips for pre-training</h3> <p>Learning rate annealing also helped. Increase first with the first 10B tokens (to 3e-4) then reduce with cosine decay to 5e-5 for the following trillions of tokens and finally reduce to 0 in the last 50B tokens (e.g. with curriculum training).</p> <p>Use efficient architectures such as Mixture of Experts.</p> <p>In terms of distributing the training across GPU’s, the recommendation is to use FSDP. Here there is an <a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">excellent tutorial</a>. One needs to ensure that the global batch size is not too large.</p> <p>Use <a href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">flash attention</a> algorithm as is faster and more memory efficient. Try to keep your code simple, before using torch.compile.</p> <p>With large training jobs it’s important to manually do garbage collection at the same time in all processes, as otherwise you can have stalls.</p> <h2 id="adaptation-or-post-training">Adaptation (or post-training)</h2> <p>The output of the pre-training is not ready for use, as it is just predicting the next token. The model needs to be adapted to the specific task at hand: this is the alignment problem: i.e. how we align the model behavior with the human preferences (or the specific task).</p> <p>The first step for adaptation is to have some target tasks (e.g. math, or writting code), with some meaningful evaluation, i.e. some specific benchmarks to evaluate. Then one needs to collect (or build) prompts that represent the task.</p> <p>Currently, in many open source LLMs, different process from <a href="https://openai.com/index/instruction-following/">Reinforcement Learning from Human Feedback</a> (RLHF), created by OpenAI, are used. In reality one can combine them, as we will see below.Some of these methods are:</p> <ul> <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a> (DPO), much simpler with similar performance. It does not need a reward model (like RLHF), and requires only to optimize a modified version of a simple binary cross entropy objective. It is used in the <a href="https://arxiv.org/pdf/2407.21783">Llama 3 model</a>.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/dpo_summary_from_paper.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/dpo_summary_from_paper.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DPO vs RLHF. From [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) paper. </div> <ul> <li> <p><a href="https://huggingface.co/blog/rishiraj/finetune-llms">Supervise Fine Tuning</a> (SFT) is a method that uses a supervised fine-tuning approach, where the model is fine-tuned on a small labeled dataset. It is used in the <a href="https://arxiv.org/pdf/2403.09611">MM1 model</a> from Apple.</p> </li> <li> <p><a href="https://www.interconnects.ai/p/tulu-3">Reinforcement Learning with Verifiable Rewards</a> (RLVR) is a quite simple but effective method coined by the authors, where they replace RLHF by a scoring function that offers positive rewards if the answer is correct. Only applicable (for now) in verifiable rewards, such as math problems with known answers.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/RL_with+VR_schema.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/RL_with+VR_schema.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Reinforcement Learning with Verifiable Rewards. From the tutorial authors. </div> <p>In terms of combining them, the authors of the tutorial suggest to start with SFT (e.g. ~1 million prompts), continue with DPO (another ~1 million prompts), and finally use Reinforcement Learning (~10k-100k prompts).</p> <h3 id="supervised-fine-tuning">Supervised Fine Tuning</h3> <p>With SFT you can get ~80% of performance gain in many tasks.</p> <p>Is used to adapt the pre-trained to specific styles of input, such as chat interactions, and can include system prompts, multi turn dialogues…</p> <p>A lot of data on this category has been created syntehtically by using LLMs to generate variations of human created prompts.</p> <p>Usually one start with mixing the existing datasets, evaluating with benchmarks, and on these benchmarks that are lagging, create new data (they usesd <a href="https://github.com/tencent-ailab/persona-hub">PersonaHub</a>).</p> <h3 id="preference-optimization-dpo">Preference Optimization (DPO)</h3> <p>Aligning to human preferences make the model stronger (e.g. ChatBotArena), allowing to control style.</p> <p>Preference optimization takes a prompt with a chosen and a rejected completion (by a human), and assumes that the probability of the chosen completion should be higher than the rejected one.</p> <p>Surprisingly low learning rates (~5E-7) are standard use. With a 70B parameter model the people from the Allen Institute were able to surpass GPT-4 in various benchmarks.</p> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p>Although more complex, it allows to normally get ~1% better performance. One can start with synthetic data (LLM-as-a-judge), that has low noise and high bias and then move to human data, with high noise but low bias.</p> <p>The leading synthetic preference data method is <a href="https://arxiv.org/abs/2310.01377v2">UltraFeedback</a>, where instructions are sampled from a large pool of models and GPT-4 is used to annotate preferences.</p> <p>They used the RLVR method, where there is no reward function but just a scoring function that offers positive rewards if the answer is correct. This is for now limited to math and precise instructions.</p> <h2 id="conclusions">Conclusions</h2> <p>The talk reminded me of other recent articles which describe the LLM building process (or at least provide some details), such as the very <a href="https://arxiv.org/pdf/2403.09611">insightful paper from Apple</a> about their MM1 model, or the <a href="https://arxiv.org/pdf/2407.21783">one from Meta on Llama 3.1</a>.</p> <p>The folks from the Allen Institute were very generous in sharing their knowledge, as I think many tips from their practical knowledge may be useful. They also shared <a href="https://github.com/allenai/awesome-open-source-lms">this repository</a> with many open source models and resources.</p> <p>I hope the video is shared soon in <a href="https://neurips.cc/virtual/2024/tutorial/99526">the tutorial page</a>.</p> <p><strong>Edit:</strong> <a href="https://www.natolambert.com/">Nathan Lambert</a> tells me that he re-recorded the last part of the tutorial, you can enjoy it here: <a href="https://www.interconnects.ai/p/the-state-of-post-training-2025">The state of post-training in 2025</a>.</p>]]></content><author><name></name></author><category term="NeurIPS"/><category term="LLM,"/><category term="NeurIPS"/><summary type="html"><![CDATA[My notes on a great tutorial at NeurIPS 2024 on how to build a Large Language Model, with many practical tips.]]></summary></entry><entry><title type="html">The path to AGI: quantifying bottlenecks</title><link href="https://manuelsh.github.io/blog/2024/the-path-to-agi-quantifying-bottlenecks/" rel="alternate" type="text/html" title="The path to AGI: quantifying bottlenecks"/><published>2024-10-06T20:03:33+00:00</published><updated>2024-10-06T20:03:33+00:00</updated><id>https://manuelsh.github.io/blog/2024/the-path-to-agi-quantifying-bottlenecks</id><content type="html" xml:base="https://manuelsh.github.io/blog/2024/the-path-to-agi-quantifying-bottlenecks/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Scaling artificial intelligence to new heights comes with significant challenges, particularly in hardware, energy, and data availability. As we strive towards Artificial General Intelligence (AGI), the hurdles grow—from the immense GPU requirements to the daunting energy consumption and even the scarcity of high-quality training data. These obstacles are demanding, yet they are not insurmountable, paving the way for ambitious innovations and new solutions.]]></summary></entry><entry><title type="html">Normalization in TensorFlow: speed is an issue</title><link href="https://manuelsh.github.io/blog/2018/normalization-in-tensorflow-speed-is-an-issue/" rel="alternate" type="text/html" title="Normalization in TensorFlow: speed is an issue"/><published>2018-02-27T10:24:39+00:00</published><updated>2018-02-27T10:24:39+00:00</updated><id>https://manuelsh.github.io/blog/2018/normalization-in-tensorflow-speed-is-an-issue</id><content type="html" xml:base="https://manuelsh.github.io/blog/2018/normalization-in-tensorflow-speed-is-an-issue/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Setting up your GPU TensorFlow platform</title><link href="https://manuelsh.github.io/blog/2017/setting-up-your-gpu-tensorflow-platform/" rel="alternate" type="text/html" title="Setting up your GPU TensorFlow platform"/><published>2017-06-11T15:10:34+00:00</published><updated>2017-06-11T15:10:34+00:00</updated><id>https://manuelsh.github.io/blog/2017/setting-up-your-gpu-tensorflow-platform</id><content type="html" xml:base="https://manuelsh.github.io/blog/2017/setting-up-your-gpu-tensorflow-platform/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">La materia al descubierto</title><link href="https://manuelsh.github.io/blog/2008/la-materia-al-descubierto/" rel="alternate" type="text/html" title="La materia al descubierto"/><published>2008-04-13T22:17:07+00:00</published><updated>2008-04-13T22:17:07+00:00</updated><id>https://manuelsh.github.io/blog/2008/la-materia-al-descubierto</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/la-materia-al-descubierto/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[En los primeros años del siglo XX se produjo una revolución extraordinaria en la física con el nacimiento de la mecánica cuántica, pero también se abrió un campo plagado de grandes interrogantes que mantienen intrigados a muchos físicos. Uno de los descubrimientos más sorprendentes fue que la luz, además de ser una onda, también se [&#8230;]]]></summary></entry><entry><title type="html">El nacimiento de la mecánica cuántica</title><link href="https://manuelsh.github.io/blog/2008/el-nacimiento-de-la-mecnica-cuntica/" rel="alternate" type="text/html" title="El nacimiento de la mecánica cuántica"/><published>2008-02-23T16:44:05+00:00</published><updated>2008-02-23T16:44:05+00:00</updated><id>https://manuelsh.github.io/blog/2008/el-nacimiento-de-la-mecnica-cuntica</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/el-nacimiento-de-la-mecnica-cuntica/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Cuando una teoría que intenta explicar la naturaleza comienza a dar resultados que no concuerdan con lo que medimos, ¡es un buen momento para cambiarla por otra nueva! Esta es la historia de cómo un truco matemático significó en realidad una nueva interpretación de la realidad: el nacimiento de la mecánica cuántica. Y ocurrió a [&#8230;]]]></summary></entry><entry><title type="html">Entendiendo la entropía</title><link href="https://manuelsh.github.io/blog/2008/entendiendo-la-entropa/" rel="alternate" type="text/html" title="Entendiendo la entropía"/><published>2008-01-27T19:26:36+00:00</published><updated>2008-01-27T19:26:36+00:00</updated><id>https://manuelsh.github.io/blog/2008/entendiendo-la-entropa</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/entendiendo-la-entropa/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[La entropía puede parecer un concepto misterioso y difícil de entender, que comúnmente se relaciona con el desorden. Sin embargo, si nos adentramos en el significado de éste, no sólo veremos que se entiende fácilmente, sino que además encierra una serie de sutilezas que lo hacen muy interesante. Antes de comenzar a explicar la entropía [&#8230;]]]></summary></entry><entry><title type="html">Dulce retorno</title><link href="https://manuelsh.github.io/blog/2007/dulce-retorno/" rel="alternate" type="text/html" title="Dulce retorno"/><published>2007-08-22T22:35:57+00:00</published><updated>2007-08-22T22:35:57+00:00</updated><id>https://manuelsh.github.io/blog/2007/dulce-retorno</id><content type="html" xml:base="https://manuelsh.github.io/blog/2007/dulce-retorno/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Tras dos semanas de periplo centroeuropeo por la elegante Viena, la enigmática Praga y la robusta y nueva Berlín, aterrizo con fuerzas renovadas. No sabría decir qué ciudad me gustó más. Creo que lo mejor sería aprovechar cada una con su configuración de historias, personas y lugares: podría vivir y trabajar en Berlín, escribir una [&#8230;]]]></summary></entry><entry><title type="html">¿Es más rentable comprar o alquilar un piso?</title><link href="https://manuelsh.github.io/blog/2007/es-ms-rentable-comprar-o-alquilar-un-piso/" rel="alternate" type="text/html" title="¿Es más rentable comprar o alquilar un piso?"/><published>2007-06-24T16:41:19+00:00</published><updated>2007-06-24T16:41:19+00:00</updated><id>https://manuelsh.github.io/blog/2007/es-ms-rentable-comprar-o-alquilar-un-piso</id><content type="html" xml:base="https://manuelsh.github.io/blog/2007/es-ms-rentable-comprar-o-alquilar-un-piso/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Desde hace un tiempo, y ante el panorama que se presenta, me hago la siguiente pregunta: ¿es más rentable comprar o alquilar un piso? Para darle respuesta he construido una pequeña tabla con el Excel en la que comparo, año a año, los costes que se van acumulando por alquilar un piso, frente a los [&#8230;]]]></summary></entry><entry><title type="html">¿Inteligencia Artificial con Google?</title><link href="https://manuelsh.github.io/blog/2007/inteligencia-artificial-con-google/" rel="alternate" type="text/html" title="¿Inteligencia Artificial con Google?"/><published>2007-05-16T20:46:22+00:00</published><updated>2007-05-16T20:46:22+00:00</updated><id>https://manuelsh.github.io/blog/2007/inteligencia-artificial-con-google</id><content type="html" xml:base="https://manuelsh.github.io/blog/2007/inteligencia-artificial-con-google/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Imagínate un programa capaz de ordenar una serie de palabras en una frase con sentido. Por ejemplo; tú le introduces: «ti, pensando, en, estoy» y él es capaz de ordenarlas y decirte: «estoy pensando en ti». Llevo varios días aprendiendo a programar en Java, .NET y a utilizar API&#8216;s para intentar hacer precisamente esto. Y [&#8230;]]]></summary></entry></feed>