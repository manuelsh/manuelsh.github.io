<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://manuelsh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://manuelsh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-23T15:03:22+00:00</updated><id>https://manuelsh.github.io/feed.xml</id><title type="html">Manuel Sánchez Hernández</title><subtitle>I like to do machine learning at scale </subtitle><entry><title type="html">Ideas from NeurIPS 2024</title><link href="https://manuelsh.github.io/blog/2025/Ideas-from-NeurIPS2024/" rel="alternate" type="text/html" title="Ideas from NeurIPS 2024"/><published>2025-01-17T12:00:00+00:00</published><updated>2025-01-17T12:00:00+00:00</updated><id>https://manuelsh.github.io/blog/2025/Ideas-from-NeurIPS2024</id><content type="html" xml:base="https://manuelsh.github.io/blog/2025/Ideas-from-NeurIPS2024/"><![CDATA[<p><a href="https://neurips.cc/Conferences/2024">NeurIPS</a> is widely considered <em>the</em> major AI research conference. With over 16,000 participants, 56 workshops, countless parallel tracks, and a staggering 3,650 posters, it’s not just an event—it’s an intellectual marathon. This year 2024, Vancouver, Canada, hosted it, and during six packed days, it offered a privileged vantage point into the state of the art in the field and their current challenges. While it’s impossible to capture its vast scope in a single post, here’s a glimpse of the most exciting ideas that stood out to me.</p> <p>I will divide it into xxx main themes: [[[put them here]]]</p> <h1 id="agents-the-next-frontier">Agents, the next frontier</h1> <p>Many researchers agree that to bring intelligent systems to the next level we need to focus on “Agents”, instead of just models (such as LLMs). This was mentioned by Ilya Sutskever in his presentations and there were many interesting presentations on the topic, including showcases of agentic libraries, like <a href="https://www.microsoft.com/en-us/research/project/autogen/">Autogen</a>, presented by Microsoft, and <a href="https://github.com/meta-llama/llama-stack">Llama Stack</a>, by the folks of Meta.</p> <h2 id="conquering-human-user-interfaces">Conquering human user interfaces</h2> <p>Very promising the <a href="https://microsoft.github.io/OmniParser/">OmniParser method</a>, also from Microsoft, which can be used to build agents able to interact with a screen or browser. In a future where our interactions with the digital world are intermediated by AI agents, they should be able to interact with the content as we do, using keyboard, mouse or tapping in a screen to navigate through user interfaces.</p> <p>For example, as I have tested myself, giving the LLM the control of the mouse and keyboard and the ability to use a browser (easily done by asking the LLM to build code with <a href="https://www.selenium.dev/">Selenium</a> or similar libraries), if you ask it to do certain tasks, like book a flight, fails miserably, with accuracies around 16% in the ScreenSpot benchmark.</p> <p>But if the model is supplemented with the input coming from Omniparser, this accuracy jumps to 73% on the same dataset. Though this increase is very significant, it has already been surpassed <a href="https://paperswithcode.com/sota/natural-language-visual-grounding-on">by other models</a>, which means that in less than one year, we will likely see AI operating seamlessly with the UI of our phones or computers as humans do.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/omniparser_example-480.webp 480w,/assets/img/blog_images/omniparser_example-800.webp 800w,/assets/img/blog_images/omniparser_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog_images/omniparser_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Example of the output of Omniparser from a screenshot with Google Slides. </div> <h2 id="other-useful-resources-about-agents">Other useful resources about Agents</h2> <p>The folks of Meta showed how to build agents with their <a href="https://github.com/meta-llama/llama-stack">Llama Stack</a>, providing also a great <a href="https://colab.research.google.com/drive/1F2ksmkoGQPa4pzRjMOE6BXWeOxWFIW6n#scrollTo=K4AvfUAJZOeS">notebook with many relevant examples</a>, which includes RAG evaluation with LLM as a judge.</p> <p>A couple of highlights were their agentic proposed architecture where a central executor orchestrates everything, as shown in the image below. Just follow the numbers in order to better understand it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/Meta_agentic_archirecture_proposal.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/Meta_agentic_archirecture_proposal.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Architecture of an agent as proposed by Meta. Source: Active Training: Building Agentic Apps with Llama 3.2 and Llama Stack. Neurips 2024. </div> <p>They also provided some hints on which model size to use for different tasks. I found the following table quite useful.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/Meta_model_sizes_and_usages_table.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/Meta_model_sizes_and_usages_table.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table showing the model sizes and their usages as proposed by Meta. Source: Active Training: Building Agentic Apps with Llama 3.2 and Llama Stack. Neurips 2024. </div> <h1 id="building-and-improving-large-language-models">Building and improving Large Language Models</h1> <p>One of the standout topics at NeurIPS this year was the process of building and improving Large Language Models. A particularly noteworthy presentation was given by the AllenAI team, who provided a detailed overview of the end-to-end process of building an LLM. From data acquisition to post-training, they shared many insights and tips. This topic is so rich that it deserves a summary of its own, which you can find <a href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/">here</a>.</p> <h2 id="are-we-running-out-of-data">Are we running out of data?</h2> <p>Many speakers touched on the topic of data scarcity. Kyle Lo from AllenAI mentioned that we are not running out of data, but of <em>open</em> data. Ilya Sutskever, in his remarks upon receiving the “Test of Time Award” for his paper, described data as the “fossil fuel of AI,” noting that while compute continues to grow, data is not growing at the same pace. He suggested that we should be looking at “synthetic data,” inference time compute, and agents as potential solutions.</p> <p>This was challenged by Jason Weston (vibrant multi-colored hair today), who pointed out that significant portion of the training of LLMs in frontier companies relies on “closed data,” which they possess and are generating in substantial quantities. He expressed skepticism about the severity of the data scarcity issue, suggesting that Ilya’s perspective might be influenced by his recent departure from OpenAI and the resulting loss of access to that data.</p> <p>It is worth mentioning the work of Epoch AI on precisely this topic. In their <a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data">Will We Run Out of Data?</a> paper they project that human public text, estimated in 300 trillion tokens, will be fully utilized between 2026 and 2032, or earlier (see chart below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/epoch-ai-data-utilization-480.webp 480w,/assets/img/blog_images/epoch-ai-data-utilization-800.webp 800w,/assets/img/blog_images/epoch-ai-data-utilization-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog_images/epoch-ai-data-utilization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: Epoch AI, June 2024 </div> <p>Epoch AI focus here on textual data. However, a significant portion of data exists in other formats, such as images, audio, and video, which can also be used for training. While computational power grows exponentially and data increases at a linear rate, advancements in algorithms and methods continue to become more efficient. Furthermore, alternatives like self-distillation (model generates data and trains with it), <a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">Constitutional AI</a>, synthetic data, and private datasets reduce this open data reliance. For all these reasons I don’t think data will be a blocker for the next years.</p> <p>Finally, consider that the most intelligent entities we know, humans, process an estimated 50-100 terabytes of raw data annually (through all our senses) with brain that consumes only ~20 Watts. This sets a benchmark on the efficiency of intelligence.</p> <h2 id="architectures-and-rl-methods">Architectures and RL methods</h2> <p>It’s clear that other alternatives to the Transformer architecture are standing out, such us <a href="https://arxiv.org/abs/2312.00752">Mamba</a> or <a href="https://arxiv.org/abs/2405.04517">xLSTM</a>. These architectures are more efficient at inference, as the computing doesn’t grow quadratically with the number of input tokens, while they can parallelize the prediction of the next token in the training, like the Transformer does, instead of sequentially like previous architectures (RNN, LSTM…). xLSTM was presented by Hochreiter, creator of the LSTM architecture, which admitted that is very similar to Mamba: “architectures converge”. However, although they were mentioned many times, they are not clear winners yet.</p> <p>Also, Reinforcement Learning with Human Feedback (or RLHF), which is the method used by OpenAI ChatGPT to make a language model a chatbot, is being substituted or supplemented by many other methods, like DPO, which is significantly easier and performs at a similar level. More details in my summary on <a href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/">Opening the LLM pipeline</a>.</p> <h1 id="measuring-the-performance-of-foundation-models">Measuring the performance of foundation models</h1> <p>Although benchmarking models is part of building models, this topic is so relevant that requires its own section. Benchmarking is important for several reasons: to understand how good a model is but also to build more intelligent models.</p> <h2 id="benchmarks-to-advance-ai">Benchmarks to advance AI</h2> <p>Intelligence definition is ellusive, that is why those benchmarks that are easy for humans but hard for AI models are very relevant. One of them is <a href="https://arcprize.org/">ARC</a>, which requires the ML model to solve a series of puzzles, each one with a different logic, like the one shown below.</p> <p>[[[[[ADD EXAMPLE!!!]]]]]</p> <p>The performance of humansin the ARC test is very high, around 90% accuracy, while at the time of Neurips 2024 the best model was at 53%. Interestingly, less than a week after François Chollet presentation in NeurIPS, OpenAI announced that their <a href="https://community.openai.com/t/day-12-of-shipmas-new-frontier-models-o3-and-o3-mini-announcement/1061818">new o3 model</a> is able to reach to 76% in ARC. A great example of how quickly the field moves!</p> <p><a href="https://melaniemitchell.me/">Melanie Mitchell</a>, from the Santa Fe Institute, also showed during a [[[[tutorial/workshop/presentation on….]]]] how current state of the art LLM fail when performing some trivial modifications on benchmarks. She mentioned an example of the insightful paper <a href="https://arxiv.org/pdf/2307.02477">Reasoning or Reciting?</a> where in a Python code benchmark, which GPT4 can do very well, just by introducing a simple change in the way the language work (“now lists index start with 1 and not zero) the model fails misserably. See the chart below. This provides a good glimpse on how far are our current best transformer models to be considered AGI.</p> <p>[[[put image from paper]]]</p> <p>In reality, building “easy for humans, hard for AI” kind of benchmarks are key to the development of more intelligent models. Indeed, as Fei Fei pointed out in her inspiring presentation, building the ImageNet benchmark was key for the rebirth of neural networks in 2012, and the newly coined term “Deep Learning”.</p> <h2 id="eureka-a-comprehensive-framework-to-evaluate-llms">EUREKA: A comprehensive framework to evaluate LLMs</h2> <p>The folks from Microsoft presented a comprehensive and open source framework to evaluate multimodal and language models called <a href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/">Eureka</a>, which assesses the performance of models across several dimensions. Some of the main conclussions of their evaluation of 14 large foundation models are:</p> <ul> <li>Models like Claude 3.5 Sonnet, GPT-4o 2024-05-13, and Llama 3.1 405B show distinct strengths in specific tasks but are not universally superior across all benchmarks. This highlights the need for task-specific analysis rather than assuming a model’s overall superiority.</li> <li>Current AI models struggle significantly with multimodal tasks, particularly those requiring detailed image understanding and spatial reasoning. For example, all models perform poorly on Object Detection. In the folllowing chart you can see the results. They are quite insightful!</li> </ul> <p>[[[put image from paper]]]</p> <h1 id="representation-learning">Representation learning</h1> <ul> <li>Very cool workshop on representation learning where they have evidence about “different neural networks, trained different, including human neural networks, converge towards the same way of representing the world”.</li> </ul> <h1 id="ai-safety">AI Safety</h1> <p>Main idea: we can get all the benefits of AI without the risks by not doing agents but special purpose models.</p> <h1 id="practical-applications">Practical applications</h1> <p>Presentation of Shopify</p> <h1 id="drama">Drama</h1> <ul> <li>Women that insulted Chinese</li> <li>Price to the main paper with guy that ….</li> </ul>]]></content><author><name></name></author><category term="NeurIPS,"/><category term="LLM"/><category term="LLM,"/><category term="NeurIPS"/><summary type="html"><![CDATA[NeurIPS 2024, the largest AI research conference, provides a glimpse into the next frontiers. Here are some of the most exciting ideas from the conference.]]></summary></entry><entry><title type="html">Opening the LLM pipeline</title><link href="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/" rel="alternate" type="text/html" title="Opening the LLM pipeline"/><published>2025-01-03T12:00:00+00:00</published><updated>2025-01-03T12:00:00+00:00</updated><id>https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop</id><content type="html" xml:base="https://manuelsh.github.io/blog/2025/NIPS-building-llm-workshop/"><![CDATA[<p>This post summarizes a fanstic tutorial about building LLM, titled <a href="https://neurips.cc/virtual/2024/tutorial/99526">“Opening the Language Model Pipeline: A Tutorial on Data Preparation, Model Training, and Adaptation”</a> (<a href="https://docs.google.com/presentation/d/179dpzWSQ9G7EAUlvaJdeE0av9PLuk9Rl33nfhHSJ4xI/edit#slide=id.g30a4c7e9678_0_0">slides</a>). It was presented at NeurIPS 2024, by <a href="https://kyleclo.com/">Kyle Lo</a>, <a href="https://akshitab.github.io/">Akshita Bhagia</a> and <a href="https://www.natolambert.com/">Nathan Lambert</a>, all from the <a href="https://allenai.org/">Allen Institute for AI</a>. I think it could be useful to share some of the main ideas.</p> <p>The process to build a Large Language Model is very involved, and the authors went through it end to end, providing many details and practical knowledge: starting with the data preparation, continuing wiht model training (also called pre-training), and adaptation (or post-training). Here I summarize the main takeaways from each part, with some additional notes added.</p> <h2 id="data-preparation">Data preparation</h2> <p>Data preparation mainly means data acquisition, data transformation (deduplication, quality control, etc) and data evaluation.</p> <h3 id="data-acquisition">Data acquisition</h3> <p>To acquire data, crawling is common. However, it’s <a href="https://www.dataprovenance.org/Consent_in_Crisis.pdf">becoming harder to crawl data</a>. Many websites are opting out or implementing anti crawlers protection, as shown in the figure below. Note that this will create a barrier to enter for new players. As Kyle Lo said: we are not running out of data, we are running out of <em>open</em> data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/crawling_data.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/crawling_data.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Longpre et. al. 2024. Consent in Crisis: The Rapid Decline of the AI Data Commons. Data Provenance Initiative. </div> <p>Crawling data from websites implies, for many of them, understanding the JavaScript logic, which in many cases is unique to the website. This can be challenging because each website may use different frameworks or obfuscation techniques, making it necessary to decipher custom implementations. For example, a site might load data dynamically through complex API calls embedded in asynchronous scripts, requiring tailored solutions for successful extraction. It also requires to parse the data from all the HTML, which is not easy. For PDF’s or scanned documents is also difficult, as many tools are not able to parse the data correctly.</p> <h3 id="data-transformation">Data transformation</h3> <p>That mostly means language filtering, deduplication, removing sensitive content (including private content) and ensuring a desired quality. In reality, it’s a classification problem, and can be done using machine learning with small classifiers. They recommend the library <a href="https://github.com/facebookresearch/fastText">fastText</a>, which is quite efficient (and used across the industry), although more involved classifiers can be used.</p> <p>They also shared the amount of data that remains once the data is filtered for quality and deduplication: reductions are usually of the order of 65 times.</p> <p>Filtering for quality can be, in some cases, problematics, as one can be undesiringly removing specific themes which are usually classified as lower quality, e.g. high school related content.</p> <p>It is also quite difficult to remove personal data, as it was highlighted by <a href="https://aclanthology.org/2023.trustnlp-1.18.pdf">Subramani et al (2023)</a>, where they showed that accuracies with simple Regex or tools like <a href="https://microsoft.github.io/presidio/">Presidio</a> can be quite low.</p> <h3 id="data-evaluation">Data evaluation</h3> <p>Finally, data must be evaluated by training models and running benchmarks, such as MMLU, HumanEval, GSM8K… The evaluation should be done systematically to each group of data, ideally starting with a smaller and cheaper mode. In general it is a very involved process with many nuances, like what is the best model size, measuring the effect of your data filtering, etc</p> <h3 id="a-new-trend-data-curriculum">A new trend: data curriculum</h3> <p>Some interesting new trend: “data curriculum”, which consists of, after training the model with trillions of tokens (high quantity, less quality), at the end of it one switches data to either very high quality sources, specific instructions or synthetic data.</p> <h2 id="model-training-or-pre-training">Model training (or pre-training)</h2> <p>Pre-training, the process where you train a LLM on next token prediction with a large amount of text, is currently mostly done with a Transformer architecture, accepting many different configurations that are successful, including many attention mechanisms (e.g. multi head, grouped-query or multi-query). See image below, how different configurations of the hyperparameters (marked in red) can lead to successful models.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/LLM_hyperparameter_configuration.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/LLM_hyperparameter_configuration.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Different configurations of the hyperparameters (marked in red) can lead to successful models. From the tutorial authors. </div> <p>In terms of scale the approximate good scaling law given by the <a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a> (compute budget approximately 6 times the number of parameters by data tokens, and data tokens approx 20 times the number of parameters), although in practice everybody keeps training further.</p> <p>In terms of costs, the pre-training can be very expensive, as it is very intensive in computational resources. See for example the table below, by the authors of the tutorials: a 7b parameter model and 150B tokens (just above the Chinchilla paper budget), will cost approx $10k.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/cost_of_LLM_training.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/cost_of_LLM_training.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Costs for different models and data sizes. From the tutorial authors. </div> <p>Common positional embeddings today are <a href="https://arxiv.org/abs/2104.09864">rotary positional embeddings</a> (RoPE) and the <a href="https://arxiv.org/abs/2002.05202v1">SwiGLU activation</a>, which, unlike ReLU activation, is smooth (differentiable) at zero.</p> <h3 id="problems-with-loss-function-convergence">Problems with loss function convergence</h3> <p>When the loss function spikes punctually, look at your data, probably is a low quality batch that needs to be filtered.</p> <p>When the loss function starts spiking and diverging, one needs to ensure that the scale of activations and gradients remain roughly the same, and they should scale with model width. Better to use normal initialization and RMSNorm, QK-Norm and change the order of the layer norm. Finally, ensure that the token embedding does not become too small (no weight decay).</p> <p>Run experiments with smaller models first, to find optimal parameters, decide on data ablations.</p> <h3 id="additional-tips-for-pre-training">Additional tips for pre-training</h3> <p>Learning rate annealing also helped. Increase first with the first 10B tokens (to 3e-4) then reduce with cosine decay to 5e-5 for the following trillions of tokens and finally reduce to 0 in the last 50B tokens (e.g. with curriculum training).</p> <p>Use efficient architectures such as Mixture of Experts.</p> <p>In terms of distributing the training across GPU’s, the recommendation is to use FSDP. Here there is an <a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">excellent tutorial</a>. One needs to ensure that the global batch size is not too large.</p> <p>Use <a href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">flash attention</a> algorithm as is faster and more memory efficient. Try to keep your code simple, before using torch.compile.</p> <p>With large training jobs it’s important to manually do garbage collection at the same time in all processes, as otherwise you can have stalls.</p> <h2 id="adaptation-or-post-training">Adaptation (or post-training)</h2> <p>The output of the pre-training is not ready for use, as it is just predicting the next token. The model needs to be adapted to the specific task at hand: this is the alignment problem: i.e. how we align the model behavior with the human preferences (or the specific task).</p> <p>The first step for adaptation is to have some target tasks (e.g. math, or writting code), with some meaningful evaluation, i.e. some specific benchmarks to evaluate. Then one needs to collect (or build) prompts that represent the task.</p> <p>Currently, in many open source LLMs, different process from <a href="https://openai.com/index/instruction-following/">Reinforcement Learning from Human Feedback</a> (RLHF), created by OpenAI, are used. In reality one can combine them, as we will see below.Some of these methods are:</p> <ul> <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a> (DPO), much simpler with similar performance. It does not need a reward model (like RLHF), and requires only to optimize a modified version of a simple binary cross entropy objective. It is used in the <a href="https://arxiv.org/pdf/2407.21783">Llama 3 model</a>.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/dpo_summary_from_paper.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/dpo_summary_from_paper.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DPO vs RLHF. From [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) paper. </div> <ul> <li> <p><a href="https://huggingface.co/blog/rishiraj/finetune-llms">Supervise Fine Tuning</a> (SFT) is a method that uses a supervised fine-tuning approach, where the model is fine-tuned on a small labeled dataset. It is used in the <a href="https://arxiv.org/pdf/2403.09611">MM1 model</a> from Apple.</p> </li> <li> <p><a href="https://www.interconnects.ai/p/tulu-3">Reinforcement Learning with Verifiable Rewards</a> (RLVR) is a quite simple but effective method coined by the authors, where they replace RLHF by a scoring function that offers positive rewards if the answer is correct. Only applicable (for now) in verifiable rewards, such as math problems with known answers.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_images/RL_with+VR_schema.JPG" sizes="95vw"/> <img src="/assets/img/blog_images/RL_with+VR_schema.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Reinforcement Learning with Verifiable Rewards. From the tutorial authors. </div> <p>In terms of combining them, the authors of the tutorial suggest to start with SFT (e.g. ~1 million prompts), continue with DPO (another ~1 million prompts), and finally use Reinforcement Learning (~10k-100k prompts).</p> <h3 id="supervised-fine-tuning">Supervised Fine Tuning</h3> <p>With SFT you can get ~80% of performance gain in many tasks.</p> <p>Is used to adapt the pre-trained to specific styles of input, such as chat interactions, and can include system prompts, multi turn dialogues…</p> <p>A lot of data on this category has been created syntehtically by using LLMs to generate variations of human created prompts.</p> <p>Usually one start with mixing the existing datasets, evaluating with benchmarks, and on these benchmarks that are lagging, create new data (they usesd <a href="https://github.com/tencent-ailab/persona-hub">PersonaHub</a>).</p> <h3 id="preference-optimization-dpo">Preference Optimization (DPO)</h3> <p>Aligning to human preferences make the model stronger (e.g. ChatBotArena), allowing to control style.</p> <p>Preference optimization takes a prompt with a chosen and a rejected completion (by a human), and assumes that the probability of the chosen completion should be higher than the rejected one.</p> <p>Surprisingly low learning rates (~5E-7) are standard use. With a 70B parameter model the people from the Allen Institute were able to surpass GPT-4 in various benchmarks.</p> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p>Although more complex, it allows to normally get ~1% better performance. One can start with synthetic data (LLM-as-a-judge), that has low noise and high bias and then move to human data, with high noise but low bias.</p> <p>The leading synthetic preference data method is <a href="https://arxiv.org/abs/2310.01377v2">UltraFeedback</a>, where instructions are sampled from a large pool of models and GPT-4 is used to annotate preferences.</p> <p>They used the RLVR method, where there is no reward function but just a scoring function that offers positive rewards if the answer is correct. This is for now limited to math and precise instructions.</p> <h2 id="conclusions">Conclusions</h2> <p>The talk reminded me of other recent articles which describe the LLM building process (or at least provide some details), such as the very <a href="https://arxiv.org/pdf/2403.09611">insightful paper from Apple</a> about their MM1 model, or the <a href="https://arxiv.org/pdf/2407.21783">one from Meta on Llama 3.1</a>.</p> <p>The folks from the Allen Institute were very generous in sharing their knowledge, as I think many tips from their practical knowledge may be useful. They also shared <a href="https://github.com/allenai/awesome-open-source-lms">this repository</a> with many open source models and resources.</p> <p>I hope the video is shared soon in <a href="https://neurips.cc/virtual/2024/tutorial/99526">the tutorial page</a>.</p> <p><strong>Edit:</strong> <a href="https://www.natolambert.com/">Nathan Lambert</a> tells me that he re-recorded the last part of the tutorial, you can enjoy it here: <a href="https://www.interconnects.ai/p/the-state-of-post-training-2025">The state of post-training in 2025</a>.</p>]]></content><author><name></name></author><category term="NeurIPS"/><category term="LLM,"/><category term="NeurIPS"/><summary type="html"><![CDATA[My notes on a great tutorial at NeurIPS 2024 on how to build a Large Language Model, with many practical tips.]]></summary></entry><entry><title type="html">The path to AGI: quantifying bottlenecks</title><link href="https://manuelsh.github.io/blog/2024/the-path-to-agi-quantifying-bottlenecks/" rel="alternate" type="text/html" title="The path to AGI: quantifying bottlenecks"/><published>2024-10-06T20:03:33+00:00</published><updated>2024-10-06T20:03:33+00:00</updated><id>https://manuelsh.github.io/blog/2024/the-path-to-agi-quantifying-bottlenecks</id><content type="html" xml:base="https://manuelsh.github.io/blog/2024/the-path-to-agi-quantifying-bottlenecks/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Scaling artificial intelligence to new heights comes with significant challenges, particularly in hardware, energy, and data availability. As we strive towards Artificial General Intelligence (AGI), the hurdles grow—from the immense GPU requirements to the daunting energy consumption and even the scarcity of high-quality training data. These obstacles are demanding, yet they are not insurmountable, paving the way for ambitious innovations and new solutions.]]></summary></entry><entry><title type="html">Normalization in TensorFlow: speed is an issue</title><link href="https://manuelsh.github.io/blog/2018/normalization-in-tensorflow-speed-is-an-issue/" rel="alternate" type="text/html" title="Normalization in TensorFlow: speed is an issue"/><published>2018-02-27T10:24:39+00:00</published><updated>2018-02-27T10:24:39+00:00</updated><id>https://manuelsh.github.io/blog/2018/normalization-in-tensorflow-speed-is-an-issue</id><content type="html" xml:base="https://manuelsh.github.io/blog/2018/normalization-in-tensorflow-speed-is-an-issue/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Setting up your GPU TensorFlow platform</title><link href="https://manuelsh.github.io/blog/2017/setting-up-your-gpu-tensorflow-platform/" rel="alternate" type="text/html" title="Setting up your GPU TensorFlow platform"/><published>2017-06-11T15:10:34+00:00</published><updated>2017-06-11T15:10:34+00:00</updated><id>https://manuelsh.github.io/blog/2017/setting-up-your-gpu-tensorflow-platform</id><content type="html" xml:base="https://manuelsh.github.io/blog/2017/setting-up-your-gpu-tensorflow-platform/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">La materia al descubierto</title><link href="https://manuelsh.github.io/blog/2008/la-materia-al-descubierto/" rel="alternate" type="text/html" title="La materia al descubierto"/><published>2008-04-13T22:17:07+00:00</published><updated>2008-04-13T22:17:07+00:00</updated><id>https://manuelsh.github.io/blog/2008/la-materia-al-descubierto</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/la-materia-al-descubierto/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[En los primeros años del siglo XX se produjo una revolución extraordinaria en la física con el nacimiento de la mecánica cuántica, pero también se abrió un campo plagado de grandes interrogantes que mantienen intrigados a muchos físicos. Uno de los descubrimientos más sorprendentes fue que la luz, además de ser una onda, también se [&#8230;]]]></summary></entry><entry><title type="html">El nacimiento de la mecánica cuántica</title><link href="https://manuelsh.github.io/blog/2008/el-nacimiento-de-la-mecnica-cuntica/" rel="alternate" type="text/html" title="El nacimiento de la mecánica cuántica"/><published>2008-02-23T16:44:05+00:00</published><updated>2008-02-23T16:44:05+00:00</updated><id>https://manuelsh.github.io/blog/2008/el-nacimiento-de-la-mecnica-cuntica</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/el-nacimiento-de-la-mecnica-cuntica/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Cuando una teoría que intenta explicar la naturaleza comienza a dar resultados que no concuerdan con lo que medimos, ¡es un buen momento para cambiarla por otra nueva! Esta es la historia de cómo un truco matemático significó en realidad una nueva interpretación de la realidad: el nacimiento de la mecánica cuántica. Y ocurrió a [&#8230;]]]></summary></entry><entry><title type="html">Entendiendo la entropía</title><link href="https://manuelsh.github.io/blog/2008/entendiendo-la-entropa/" rel="alternate" type="text/html" title="Entendiendo la entropía"/><published>2008-01-27T19:26:36+00:00</published><updated>2008-01-27T19:26:36+00:00</updated><id>https://manuelsh.github.io/blog/2008/entendiendo-la-entropa</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/entendiendo-la-entropa/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[La entropía puede parecer un concepto misterioso y difícil de entender, que comúnmente se relaciona con el desorden. Sin embargo, si nos adentramos en el significado de éste, no sólo veremos que se entiende fácilmente, sino que además encierra una serie de sutilezas que lo hacen muy interesante. Antes de comenzar a explicar la entropía [&#8230;]]]></summary></entry><entry><title type="html">Tautología</title><link href="https://manuelsh.github.io/blog/2008/tautologa/" rel="alternate" type="text/html" title="Tautología"/><published>2008-01-07T20:30:13+00:00</published><updated>2008-01-07T20:30:13+00:00</updated><id>https://manuelsh.github.io/blog/2008/tautologa</id><content type="html" xml:base="https://manuelsh.github.io/blog/2008/tautologa/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Es insulso no ser creativo. De hecho, lo es, por definición.]]></summary></entry><entry><title type="html">Distribución de visitas de Inquietudes</title><link href="https://manuelsh.github.io/blog/2007/distribucin-de-visitas-de-inquietudes/" rel="alternate" type="text/html" title="Distribución de visitas de Inquietudes"/><published>2007-09-21T18:34:40+00:00</published><updated>2007-09-21T18:34:40+00:00</updated><id>https://manuelsh.github.io/blog/2007/distribucin-de-visitas-de-inquietudes</id><content type="html" xml:base="https://manuelsh.github.io/blog/2007/distribucin-de-visitas-de-inquietudes/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[En el post sobre el record de visitas de Agosto, mi hermano Dani me hizo un inspirador comentario que me llevó a la siguiente pregunta: ¿cómo se distribuyen las visitas a lo largo de los posts de Inquietudes? Con un poco de Excel y paciencia, obtuve la respuesta: ¿Qué significa esta gráfica? Aquí se representa [&#8230;]]]></summary></entry></feed>